2020-07-07 20:00:24,391 ERROR An error occurred while calling o174.approxQuantile.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 441, localhost, executor driver): org.apache.spark.sql.avro.IncompatibleSchemaException: Cannot convert Avro to catalyst because schema at path gross_amount is not compatible (avroType = "double", sqlType = FloatType).
Source Avro schema: {"type":"record","name":"topLevelRecord","fields":[{"name":"table_id","type":["long","null"]},{"name":"track_id","type":["string","null"]},{"name":"customer_id","type":["string","null"]},{"name":"customer_key","type":["string","null"]},{"name":"customer_type","type":"string"},{"name":"servicecode","type":["string","null"]},{"name":"serviceamount","type":["float","null"]},{"name":"quantity","type":["float","null"]},{"name":"net_amount","type":["float","null"]},{"name":"discount","type":["float","null"]},{"name":"gross_amount","type":["double","null"]},{"name":"currency","type":["string","null"]},{"name":"trasactionstype","type":["string","null"]},{"name":"tranx_date","type":["string","null"]},{"name":"promo","type":["string","null"]},{"name":"paymentchannelcode","type":["string","null"]},{"name":"paymentchannelname","type":["string","null"]},{"name":"paymentreference","type":["string","null"]},{"name":"servicereference","type":["string","null"]},{"name":"id_udid","type":["string","null"]},{"name":"app_type","type":["string","null"]},{"name":"industry","type":"string"},{"name":"Branch","type":"string"},{"name":"status","type":"string"},{"name":"quickpay_trackid","type":["string","null"]}]}.
Target Catalyst type: StructType(StructField(customer_id,StringType,true), StructField(gross_amount,FloatType,true), StructField(tranx_date,StringType,true))
	at org.apache.spark.sql.avro.AvroDeserializer.org$apache$spark$sql$avro$AvroDeserializer$$newWriter(AvroDeserializer.scala:275)
	at org.apache.spark.sql.avro.AvroDeserializer.getRecordWriter(AvroDeserializer.scala:308)
	at org.apache.spark.sql.avro.AvroDeserializer.<init>(AvroDeserializer.scala:53)
	at org.apache.spark.sql.avro.AvroFileFormat$$anonfun$buildReader$1.apply(AvroFileFormat.scala:215)
	at org.apache.spark.sql.avro.AvroFileFormat$$anonfun$buildReader$1.apply(AvroFileFormat.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)
	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1143)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
	at org.apache.spark.rdd.RDD.fold(RDD.scala:1137)
	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1206)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
	at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1182)
	at org.apache.spark.sql.execution.stat.StatFunctions$.multipleApproxQuantiles(StatFunctions.scala:102)
	at org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:100)
	at org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:115)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.avro.IncompatibleSchemaException: Cannot convert Avro to catalyst because schema at path gross_amount is not compatible (avroType = "double", sqlType = FloatType).
Source Avro schema: {"type":"record","name":"topLevelRecord","fields":[{"name":"table_id","type":["long","null"]},{"name":"track_id","type":["string","null"]},{"name":"customer_id","type":["string","null"]},{"name":"customer_key","type":["string","null"]},{"name":"customer_type","type":"string"},{"name":"servicecode","type":["string","null"]},{"name":"serviceamount","type":["float","null"]},{"name":"quantity","type":["float","null"]},{"name":"net_amount","type":["float","null"]},{"name":"discount","type":["float","null"]},{"name":"gross_amount","type":["double","null"]},{"name":"currency","type":["string","null"]},{"name":"trasactionstype","type":["string","null"]},{"name":"tranx_date","type":["string","null"]},{"name":"promo","type":["string","null"]},{"name":"paymentchannelcode","type":["string","null"]},{"name":"paymentchannelname","type":["string","null"]},{"name":"paymentreference","type":["string","null"]},{"name":"servicereference","type":["string","null"]},{"name":"id_udid","type":["string","null"]},{"name":"app_type","type":["string","null"]},{"name":"industry","type":"string"},{"name":"Branch","type":"string"},{"name":"status","type":"string"},{"name":"quickpay_trackid","type":["string","null"]}]}.
Target Catalyst type: StructType(StructField(customer_id,StringType,true), StructField(gross_amount,FloatType,true), StructField(tranx_date,StringType,true))
	at org.apache.spark.sql.avro.AvroDeserializer.org$apache$spark$sql$avro$AvroDeserializer$$newWriter(AvroDeserializer.scala:275)
	at org.apache.spark.sql.avro.AvroDeserializer.getRecordWriter(AvroDeserializer.scala:308)
	at org.apache.spark.sql.avro.AvroDeserializer.<init>(AvroDeserializer.scala:53)
	at org.apache.spark.sql.avro.AvroFileFormat$$anonfun$buildReader$1.apply(AvroFileFormat.scala:215)
	at org.apache.spark.sql.avro.AvroFileFormat$$anonfun$buildReader$1.apply(AvroFileFormat.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)
	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Traceback (most recent call last):
  File "G:/Users/csaiamogh/PycharmProjects/RFM_Spark/src/main/python/main.py", line 96, in main
    data = rfm_object.transformation()
  File "src/main\rfm_modules\rfm_functions.py", line 99, in transformation
    monetary_value_quantile = data.approxQuantile('monetary_value', percentiles, 0)
  File "C:\spark-2.4.6-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\dataframe.py", line 1892, in approxQuantile
    jaq = self._jdf.stat().approxQuantile(col, probabilities, relativeError)
  File "C:\spark-2.4.6-bin-hadoop2.7\python\lib\py4j-0.10.7-src.zip\py4j\java_gateway.py", line 1257, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "C:\spark-2.4.6-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\utils.py", line 63, in deco
    return f(*a, **kw)
  File "C:\spark-2.4.6-bin-hadoop2.7\python\lib\py4j-0.10.7-src.zip\py4j\protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o174.approxQuantile.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 441, localhost, executor driver): org.apache.spark.sql.avro.IncompatibleSchemaException: Cannot convert Avro to catalyst because schema at path gross_amount is not compatible (avroType = "double", sqlType = FloatType).
Source Avro schema: {"type":"record","name":"topLevelRecord","fields":[{"name":"table_id","type":["long","null"]},{"name":"track_id","type":["string","null"]},{"name":"customer_id","type":["string","null"]},{"name":"customer_key","type":["string","null"]},{"name":"customer_type","type":"string"},{"name":"servicecode","type":["string","null"]},{"name":"serviceamount","type":["float","null"]},{"name":"quantity","type":["float","null"]},{"name":"net_amount","type":["float","null"]},{"name":"discount","type":["float","null"]},{"name":"gross_amount","type":["double","null"]},{"name":"currency","type":["string","null"]},{"name":"trasactionstype","type":["string","null"]},{"name":"tranx_date","type":["string","null"]},{"name":"promo","type":["string","null"]},{"name":"paymentchannelcode","type":["string","null"]},{"name":"paymentchannelname","type":["string","null"]},{"name":"paymentreference","type":["string","null"]},{"name":"servicereference","type":["string","null"]},{"name":"id_udid","type":["string","null"]},{"name":"app_type","type":["string","null"]},{"name":"industry","type":"string"},{"name":"Branch","type":"string"},{"name":"status","type":"string"},{"name":"quickpay_trackid","type":["string","null"]}]}.
Target Catalyst type: StructType(StructField(customer_id,StringType,true), StructField(gross_amount,FloatType,true), StructField(tranx_date,StringType,true))
	at org.apache.spark.sql.avro.AvroDeserializer.org$apache$spark$sql$avro$AvroDeserializer$$newWriter(AvroDeserializer.scala:275)
	at org.apache.spark.sql.avro.AvroDeserializer.getRecordWriter(AvroDeserializer.scala:308)
	at org.apache.spark.sql.avro.AvroDeserializer.<init>(AvroDeserializer.scala:53)
	at org.apache.spark.sql.avro.AvroFileFormat$$anonfun$buildReader$1.apply(AvroFileFormat.scala:215)
	at org.apache.spark.sql.avro.AvroFileFormat$$anonfun$buildReader$1.apply(AvroFileFormat.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)
	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1143)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
	at org.apache.spark.rdd.RDD.fold(RDD.scala:1137)
	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1206)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
	at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1182)
	at org.apache.spark.sql.execution.stat.StatFunctions$.multipleApproxQuantiles(StatFunctions.scala:102)
	at org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:100)
	at org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:115)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.avro.IncompatibleSchemaException: Cannot convert Avro to catalyst because schema at path gross_amount is not compatible (avroType = "double", sqlType = FloatType).
Source Avro schema: {"type":"record","name":"topLevelRecord","fields":[{"name":"table_id","type":["long","null"]},{"name":"track_id","type":["string","null"]},{"name":"customer_id","type":["string","null"]},{"name":"customer_key","type":["string","null"]},{"name":"customer_type","type":"string"},{"name":"servicecode","type":["string","null"]},{"name":"serviceamount","type":["float","null"]},{"name":"quantity","type":["float","null"]},{"name":"net_amount","type":["float","null"]},{"name":"discount","type":["float","null"]},{"name":"gross_amount","type":["double","null"]},{"name":"currency","type":["string","null"]},{"name":"trasactionstype","type":["string","null"]},{"name":"tranx_date","type":["string","null"]},{"name":"promo","type":["string","null"]},{"name":"paymentchannelcode","type":["string","null"]},{"name":"paymentchannelname","type":["string","null"]},{"name":"paymentreference","type":["string","null"]},{"name":"servicereference","type":["string","null"]},{"name":"id_udid","type":["string","null"]},{"name":"app_type","type":["string","null"]},{"name":"industry","type":"string"},{"name":"Branch","type":"string"},{"name":"status","type":"string"},{"name":"quickpay_trackid","type":["string","null"]}]}.
Target Catalyst type: StructType(StructField(customer_id,StringType,true), StructField(gross_amount,FloatType,true), StructField(tranx_date,StringType,true))
	at org.apache.spark.sql.avro.AvroDeserializer.org$apache$spark$sql$avro$AvroDeserializer$$newWriter(AvroDeserializer.scala:275)
	at org.apache.spark.sql.avro.AvroDeserializer.getRecordWriter(AvroDeserializer.scala:308)
	at org.apache.spark.sql.avro.AvroDeserializer.<init>(AvroDeserializer.scala:53)
	at org.apache.spark.sql.avro.AvroFileFormat$$anonfun$buildReader$1.apply(AvroFileFormat.scala:215)
	at org.apache.spark.sql.avro.AvroFileFormat$$anonfun$buildReader$1.apply(AvroFileFormat.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)
	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2020-07-07 20:00:24,397 ERROR local variable 'data' referenced before assignment
Traceback (most recent call last):
  File "G:/Users/csaiamogh/PycharmProjects/RFM_Spark/src/main/python/main.py", line 108, in main
    write_obj.write_csv(df = data ,mode ='append',path = '/home/csai/Desktop/og_helicalinsight/data/customer_level_test/rfm_product_20191001-20191231/')
UnboundLocalError: local variable 'data' referenced before assignment
2020-07-07 20:00:24,397 ERROR local variable 'data_agg' referenced before assignment
Traceback (most recent call last):
  File "G:/Users/csaiamogh/PycharmProjects/RFM_Spark/src/main/python/main.py", line 112, in main
    write_obj.write_csv(df=data_agg, mode='append',
UnboundLocalError: local variable 'data_agg' referenced before assignment
2020-07-07 20:06:30,071 ERROR An error occurred while calling o174.approxQuantile.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 441, localhost, executor driver): org.apache.spark.sql.avro.IncompatibleSchemaException: Cannot convert Avro to catalyst because schema at path gross_amount is not compatible (avroType = "double", sqlType = FloatType).
Source Avro schema: {"type":"record","name":"topLevelRecord","fields":[{"name":"table_id","type":["long","null"]},{"name":"track_id","type":["string","null"]},{"name":"customer_id","type":["string","null"]},{"name":"customer_key","type":["string","null"]},{"name":"customer_type","type":"string"},{"name":"servicecode","type":["string","null"]},{"name":"serviceamount","type":["float","null"]},{"name":"quantity","type":["float","null"]},{"name":"net_amount","type":["float","null"]},{"name":"discount","type":["float","null"]},{"name":"gross_amount","type":["double","null"]},{"name":"currency","type":["string","null"]},{"name":"trasactionstype","type":["string","null"]},{"name":"tranx_date","type":["string","null"]},{"name":"promo","type":["string","null"]},{"name":"paymentchannelcode","type":["string","null"]},{"name":"paymentchannelname","type":["string","null"]},{"name":"paymentreference","type":["string","null"]},{"name":"servicereference","type":["string","null"]},{"name":"id_udid","type":["string","null"]},{"name":"app_type","type":["string","null"]},{"name":"industry","type":"string"},{"name":"Branch","type":"string"},{"name":"status","type":"string"},{"name":"quickpay_trackid","type":["string","null"]}]}.
Target Catalyst type: StructType(StructField(customer_id,StringType,true), StructField(gross_amount,FloatType,true), StructField(tranx_date,StringType,true))
	at org.apache.spark.sql.avro.AvroDeserializer.org$apache$spark$sql$avro$AvroDeserializer$$newWriter(AvroDeserializer.scala:275)
	at org.apache.spark.sql.avro.AvroDeserializer.getRecordWriter(AvroDeserializer.scala:308)
	at org.apache.spark.sql.avro.AvroDeserializer.<init>(AvroDeserializer.scala:53)
	at org.apache.spark.sql.avro.AvroFileFormat$$anonfun$buildReader$1.apply(AvroFileFormat.scala:215)
	at org.apache.spark.sql.avro.AvroFileFormat$$anonfun$buildReader$1.apply(AvroFileFormat.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)
	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1143)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
	at org.apache.spark.rdd.RDD.fold(RDD.scala:1137)
	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1206)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
	at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1182)
	at org.apache.spark.sql.execution.stat.StatFunctions$.multipleApproxQuantiles(StatFunctions.scala:102)
	at org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:100)
	at org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:115)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.avro.IncompatibleSchemaException: Cannot convert Avro to catalyst because schema at path gross_amount is not compatible (avroType = "double", sqlType = FloatType).
Source Avro schema: {"type":"record","name":"topLevelRecord","fields":[{"name":"table_id","type":["long","null"]},{"name":"track_id","type":["string","null"]},{"name":"customer_id","type":["string","null"]},{"name":"customer_key","type":["string","null"]},{"name":"customer_type","type":"string"},{"name":"servicecode","type":["string","null"]},{"name":"serviceamount","type":["float","null"]},{"name":"quantity","type":["float","null"]},{"name":"net_amount","type":["float","null"]},{"name":"discount","type":["float","null"]},{"name":"gross_amount","type":["double","null"]},{"name":"currency","type":["string","null"]},{"name":"trasactionstype","type":["string","null"]},{"name":"tranx_date","type":["string","null"]},{"name":"promo","type":["string","null"]},{"name":"paymentchannelcode","type":["string","null"]},{"name":"paymentchannelname","type":["string","null"]},{"name":"paymentreference","type":["string","null"]},{"name":"servicereference","type":["string","null"]},{"name":"id_udid","type":["string","null"]},{"name":"app_type","type":["string","null"]},{"name":"industry","type":"string"},{"name":"Branch","type":"string"},{"name":"status","type":"string"},{"name":"quickpay_trackid","type":["string","null"]}]}.
Target Catalyst type: StructType(StructField(customer_id,StringType,true), StructField(gross_amount,FloatType,true), StructField(tranx_date,StringType,true))
	at org.apache.spark.sql.avro.AvroDeserializer.org$apache$spark$sql$avro$AvroDeserializer$$newWriter(AvroDeserializer.scala:275)
	at org.apache.spark.sql.avro.AvroDeserializer.getRecordWriter(AvroDeserializer.scala:308)
	at org.apache.spark.sql.avro.AvroDeserializer.<init>(AvroDeserializer.scala:53)
	at org.apache.spark.sql.avro.AvroFileFormat$$anonfun$buildReader$1.apply(AvroFileFormat.scala:215)
	at org.apache.spark.sql.avro.AvroFileFormat$$anonfun$buildReader$1.apply(AvroFileFormat.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)
	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Traceback (most recent call last):
  File "G:/Users/csaiamogh/PycharmProjects/RFM_Spark/src/main/python/main.py", line 96, in main
    data = rfm_object.transformation()
  File "src/main\rfm_modules\rfm_functions.py", line 99, in transformation
    monetary_value_quantile = data.approxQuantile('monetary_value', percentiles, 0)
  File "C:\spark-2.4.6-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\dataframe.py", line 1892, in approxQuantile
    jaq = self._jdf.stat().approxQuantile(col, probabilities, relativeError)
  File "C:\spark-2.4.6-bin-hadoop2.7\python\lib\py4j-0.10.7-src.zip\py4j\java_gateway.py", line 1257, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "C:\spark-2.4.6-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\utils.py", line 63, in deco
    return f(*a, **kw)
  File "C:\spark-2.4.6-bin-hadoop2.7\python\lib\py4j-0.10.7-src.zip\py4j\protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o174.approxQuantile.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 441, localhost, executor driver): org.apache.spark.sql.avro.IncompatibleSchemaException: Cannot convert Avro to catalyst because schema at path gross_amount is not compatible (avroType = "double", sqlType = FloatType).
Source Avro schema: {"type":"record","name":"topLevelRecord","fields":[{"name":"table_id","type":["long","null"]},{"name":"track_id","type":["string","null"]},{"name":"customer_id","type":["string","null"]},{"name":"customer_key","type":["string","null"]},{"name":"customer_type","type":"string"},{"name":"servicecode","type":["string","null"]},{"name":"serviceamount","type":["float","null"]},{"name":"quantity","type":["float","null"]},{"name":"net_amount","type":["float","null"]},{"name":"discount","type":["float","null"]},{"name":"gross_amount","type":["double","null"]},{"name":"currency","type":["string","null"]},{"name":"trasactionstype","type":["string","null"]},{"name":"tranx_date","type":["string","null"]},{"name":"promo","type":["string","null"]},{"name":"paymentchannelcode","type":["string","null"]},{"name":"paymentchannelname","type":["string","null"]},{"name":"paymentreference","type":["string","null"]},{"name":"servicereference","type":["string","null"]},{"name":"id_udid","type":["string","null"]},{"name":"app_type","type":["string","null"]},{"name":"industry","type":"string"},{"name":"Branch","type":"string"},{"name":"status","type":"string"},{"name":"quickpay_trackid","type":["string","null"]}]}.
Target Catalyst type: StructType(StructField(customer_id,StringType,true), StructField(gross_amount,FloatType,true), StructField(tranx_date,StringType,true))
	at org.apache.spark.sql.avro.AvroDeserializer.org$apache$spark$sql$avro$AvroDeserializer$$newWriter(AvroDeserializer.scala:275)
	at org.apache.spark.sql.avro.AvroDeserializer.getRecordWriter(AvroDeserializer.scala:308)
	at org.apache.spark.sql.avro.AvroDeserializer.<init>(AvroDeserializer.scala:53)
	at org.apache.spark.sql.avro.AvroFileFormat$$anonfun$buildReader$1.apply(AvroFileFormat.scala:215)
	at org.apache.spark.sql.avro.AvroFileFormat$$anonfun$buildReader$1.apply(AvroFileFormat.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)
	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1143)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
	at org.apache.spark.rdd.RDD.fold(RDD.scala:1137)
	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1206)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
	at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1182)
	at org.apache.spark.sql.execution.stat.StatFunctions$.multipleApproxQuantiles(StatFunctions.scala:102)
	at org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:100)
	at org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:115)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.avro.IncompatibleSchemaException: Cannot convert Avro to catalyst because schema at path gross_amount is not compatible (avroType = "double", sqlType = FloatType).
Source Avro schema: {"type":"record","name":"topLevelRecord","fields":[{"name":"table_id","type":["long","null"]},{"name":"track_id","type":["string","null"]},{"name":"customer_id","type":["string","null"]},{"name":"customer_key","type":["string","null"]},{"name":"customer_type","type":"string"},{"name":"servicecode","type":["string","null"]},{"name":"serviceamount","type":["float","null"]},{"name":"quantity","type":["float","null"]},{"name":"net_amount","type":["float","null"]},{"name":"discount","type":["float","null"]},{"name":"gross_amount","type":["double","null"]},{"name":"currency","type":["string","null"]},{"name":"trasactionstype","type":["string","null"]},{"name":"tranx_date","type":["string","null"]},{"name":"promo","type":["string","null"]},{"name":"paymentchannelcode","type":["string","null"]},{"name":"paymentchannelname","type":["string","null"]},{"name":"paymentreference","type":["string","null"]},{"name":"servicereference","type":["string","null"]},{"name":"id_udid","type":["string","null"]},{"name":"app_type","type":["string","null"]},{"name":"industry","type":"string"},{"name":"Branch","type":"string"},{"name":"status","type":"string"},{"name":"quickpay_trackid","type":["string","null"]}]}.
Target Catalyst type: StructType(StructField(customer_id,StringType,true), StructField(gross_amount,FloatType,true), StructField(tranx_date,StringType,true))
	at org.apache.spark.sql.avro.AvroDeserializer.org$apache$spark$sql$avro$AvroDeserializer$$newWriter(AvroDeserializer.scala:275)
	at org.apache.spark.sql.avro.AvroDeserializer.getRecordWriter(AvroDeserializer.scala:308)
	at org.apache.spark.sql.avro.AvroDeserializer.<init>(AvroDeserializer.scala:53)
	at org.apache.spark.sql.avro.AvroFileFormat$$anonfun$buildReader$1.apply(AvroFileFormat.scala:215)
	at org.apache.spark.sql.avro.AvroFileFormat$$anonfun$buildReader$1.apply(AvroFileFormat.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)
	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2020-07-07 20:06:30,077 ERROR local variable 'data' referenced before assignment
Traceback (most recent call last):
  File "G:/Users/csaiamogh/PycharmProjects/RFM_Spark/src/main/python/main.py", line 108, in main
    write_obj.write_csv(df = data ,mode ='append',path = '/E:/linux/backup/Documents/og_tranx/customer_level_test/rfm_product_20191001-20191231/')
UnboundLocalError: local variable 'data' referenced before assignment
2020-07-07 20:06:30,078 ERROR local variable 'data_agg' referenced before assignment
Traceback (most recent call last):
  File "G:/Users/csaiamogh/PycharmProjects/RFM_Spark/src/main/python/main.py", line 112, in main
    write_obj.write_csv(df=data_agg, mode='append',
UnboundLocalError: local variable 'data_agg' referenced before assignment
2020-07-07 20:08:10,153 ERROR An error occurred while calling o170.approxQuantile.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 441, localhost, executor driver): org.apache.spark.sql.avro.IncompatibleSchemaException: Cannot convert Avro to catalyst because schema at path gross_amount is not compatible (avroType = "double", sqlType = FloatType).
Source Avro schema: {"type":"record","name":"topLevelRecord","fields":[{"name":"table_id","type":["long","null"]},{"name":"track_id","type":["string","null"]},{"name":"customer_id","type":["string","null"]},{"name":"customer_key","type":["string","null"]},{"name":"customer_type","type":"string"},{"name":"servicecode","type":["string","null"]},{"name":"serviceamount","type":["float","null"]},{"name":"quantity","type":["float","null"]},{"name":"net_amount","type":["float","null"]},{"name":"discount","type":["float","null"]},{"name":"gross_amount","type":["double","null"]},{"name":"currency","type":["string","null"]},{"name":"trasactionstype","type":["string","null"]},{"name":"tranx_date","type":["string","null"]},{"name":"promo","type":["string","null"]},{"name":"paymentchannelcode","type":["string","null"]},{"name":"paymentchannelname","type":["string","null"]},{"name":"paymentreference","type":["string","null"]},{"name":"servicereference","type":["string","null"]},{"name":"id_udid","type":["string","null"]},{"name":"app_type","type":["string","null"]},{"name":"industry","type":"string"},{"name":"Branch","type":"string"},{"name":"status","type":"string"},{"name":"quickpay_trackid","type":["string","null"]}]}.
Target Catalyst type: StructType(StructField(customer_id,StringType,true), StructField(gross_amount,FloatType,true), StructField(tranx_date,StringType,true))
	at org.apache.spark.sql.avro.AvroDeserializer.org$apache$spark$sql$avro$AvroDeserializer$$newWriter(AvroDeserializer.scala:275)
	at org.apache.spark.sql.avro.AvroDeserializer.getRecordWriter(AvroDeserializer.scala:308)
	at org.apache.spark.sql.avro.AvroDeserializer.<init>(AvroDeserializer.scala:53)
	at org.apache.spark.sql.avro.AvroFileFormat$$anonfun$buildReader$1.apply(AvroFileFormat.scala:215)
	at org.apache.spark.sql.avro.AvroFileFormat$$anonfun$buildReader$1.apply(AvroFileFormat.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)
	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1143)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
	at org.apache.spark.rdd.RDD.fold(RDD.scala:1137)
	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1206)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
	at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1182)
	at org.apache.spark.sql.execution.stat.StatFunctions$.multipleApproxQuantiles(StatFunctions.scala:102)
	at org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:100)
	at org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:115)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.avro.IncompatibleSchemaException: Cannot convert Avro to catalyst because schema at path gross_amount is not compatible (avroType = "double", sqlType = FloatType).
Source Avro schema: {"type":"record","name":"topLevelRecord","fields":[{"name":"table_id","type":["long","null"]},{"name":"track_id","type":["string","null"]},{"name":"customer_id","type":["string","null"]},{"name":"customer_key","type":["string","null"]},{"name":"customer_type","type":"string"},{"name":"servicecode","type":["string","null"]},{"name":"serviceamount","type":["float","null"]},{"name":"quantity","type":["float","null"]},{"name":"net_amount","type":["float","null"]},{"name":"discount","type":["float","null"]},{"name":"gross_amount","type":["double","null"]},{"name":"currency","type":["string","null"]},{"name":"trasactionstype","type":["string","null"]},{"name":"tranx_date","type":["string","null"]},{"name":"promo","type":["string","null"]},{"name":"paymentchannelcode","type":["string","null"]},{"name":"paymentchannelname","type":["string","null"]},{"name":"paymentreference","type":["string","null"]},{"name":"servicereference","type":["string","null"]},{"name":"id_udid","type":["string","null"]},{"name":"app_type","type":["string","null"]},{"name":"industry","type":"string"},{"name":"Branch","type":"string"},{"name":"status","type":"string"},{"name":"quickpay_trackid","type":["string","null"]}]}.
Target Catalyst type: StructType(StructField(customer_id,StringType,true), StructField(gross_amount,FloatType,true), StructField(tranx_date,StringType,true))
	at org.apache.spark.sql.avro.AvroDeserializer.org$apache$spark$sql$avro$AvroDeserializer$$newWriter(AvroDeserializer.scala:275)
	at org.apache.spark.sql.avro.AvroDeserializer.getRecordWriter(AvroDeserializer.scala:308)
	at org.apache.spark.sql.avro.AvroDeserializer.<init>(AvroDeserializer.scala:53)
	at org.apache.spark.sql.avro.AvroFileFormat$$anonfun$buildReader$1.apply(AvroFileFormat.scala:215)
	at org.apache.spark.sql.avro.AvroFileFormat$$anonfun$buildReader$1.apply(AvroFileFormat.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)
	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Traceback (most recent call last):
  File "G:/Users/csaiamogh/PycharmProjects/RFM_Spark/src/main/python/main.py", line 96, in main
    data = rfm_object.transformation()
  File "src/main\rfm_modules\rfm_functions.py", line 99, in transformation
    monetary_value_quantile = data.approxQuantile('monetary_value', percentiles, 0)
  File "C:\spark-2.4.6-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\dataframe.py", line 1892, in approxQuantile
    jaq = self._jdf.stat().approxQuantile(col, probabilities, relativeError)
  File "C:\spark-2.4.6-bin-hadoop2.7\python\lib\py4j-0.10.7-src.zip\py4j\java_gateway.py", line 1257, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "C:\spark-2.4.6-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\utils.py", line 63, in deco
    return f(*a, **kw)
  File "C:\spark-2.4.6-bin-hadoop2.7\python\lib\py4j-0.10.7-src.zip\py4j\protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o170.approxQuantile.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 441, localhost, executor driver): org.apache.spark.sql.avro.IncompatibleSchemaException: Cannot convert Avro to catalyst because schema at path gross_amount is not compatible (avroType = "double", sqlType = FloatType).
Source Avro schema: {"type":"record","name":"topLevelRecord","fields":[{"name":"table_id","type":["long","null"]},{"name":"track_id","type":["string","null"]},{"name":"customer_id","type":["string","null"]},{"name":"customer_key","type":["string","null"]},{"name":"customer_type","type":"string"},{"name":"servicecode","type":["string","null"]},{"name":"serviceamount","type":["float","null"]},{"name":"quantity","type":["float","null"]},{"name":"net_amount","type":["float","null"]},{"name":"discount","type":["float","null"]},{"name":"gross_amount","type":["double","null"]},{"name":"currency","type":["string","null"]},{"name":"trasactionstype","type":["string","null"]},{"name":"tranx_date","type":["string","null"]},{"name":"promo","type":["string","null"]},{"name":"paymentchannelcode","type":["string","null"]},{"name":"paymentchannelname","type":["string","null"]},{"name":"paymentreference","type":["string","null"]},{"name":"servicereference","type":["string","null"]},{"name":"id_udid","type":["string","null"]},{"name":"app_type","type":["string","null"]},{"name":"industry","type":"string"},{"name":"Branch","type":"string"},{"name":"status","type":"string"},{"name":"quickpay_trackid","type":["string","null"]}]}.
Target Catalyst type: StructType(StructField(customer_id,StringType,true), StructField(gross_amount,FloatType,true), StructField(tranx_date,StringType,true))
	at org.apache.spark.sql.avro.AvroDeserializer.org$apache$spark$sql$avro$AvroDeserializer$$newWriter(AvroDeserializer.scala:275)
	at org.apache.spark.sql.avro.AvroDeserializer.getRecordWriter(AvroDeserializer.scala:308)
	at org.apache.spark.sql.avro.AvroDeserializer.<init>(AvroDeserializer.scala:53)
	at org.apache.spark.sql.avro.AvroFileFormat$$anonfun$buildReader$1.apply(AvroFileFormat.scala:215)
	at org.apache.spark.sql.avro.AvroFileFormat$$anonfun$buildReader$1.apply(AvroFileFormat.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)
	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1143)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
	at org.apache.spark.rdd.RDD.fold(RDD.scala:1137)
	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1206)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
	at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1182)
	at org.apache.spark.sql.execution.stat.StatFunctions$.multipleApproxQuantiles(StatFunctions.scala:102)
	at org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:100)
	at org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:115)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.avro.IncompatibleSchemaException: Cannot convert Avro to catalyst because schema at path gross_amount is not compatible (avroType = "double", sqlType = FloatType).
Source Avro schema: {"type":"record","name":"topLevelRecord","fields":[{"name":"table_id","type":["long","null"]},{"name":"track_id","type":["string","null"]},{"name":"customer_id","type":["string","null"]},{"name":"customer_key","type":["string","null"]},{"name":"customer_type","type":"string"},{"name":"servicecode","type":["string","null"]},{"name":"serviceamount","type":["float","null"]},{"name":"quantity","type":["float","null"]},{"name":"net_amount","type":["float","null"]},{"name":"discount","type":["float","null"]},{"name":"gross_amount","type":["double","null"]},{"name":"currency","type":["string","null"]},{"name":"trasactionstype","type":["string","null"]},{"name":"tranx_date","type":["string","null"]},{"name":"promo","type":["string","null"]},{"name":"paymentchannelcode","type":["string","null"]},{"name":"paymentchannelname","type":["string","null"]},{"name":"paymentreference","type":["string","null"]},{"name":"servicereference","type":["string","null"]},{"name":"id_udid","type":["string","null"]},{"name":"app_type","type":["string","null"]},{"name":"industry","type":"string"},{"name":"Branch","type":"string"},{"name":"status","type":"string"},{"name":"quickpay_trackid","type":["string","null"]}]}.
Target Catalyst type: StructType(StructField(customer_id,StringType,true), StructField(gross_amount,FloatType,true), StructField(tranx_date,StringType,true))
	at org.apache.spark.sql.avro.AvroDeserializer.org$apache$spark$sql$avro$AvroDeserializer$$newWriter(AvroDeserializer.scala:275)
	at org.apache.spark.sql.avro.AvroDeserializer.getRecordWriter(AvroDeserializer.scala:308)
	at org.apache.spark.sql.avro.AvroDeserializer.<init>(AvroDeserializer.scala:53)
	at org.apache.spark.sql.avro.AvroFileFormat$$anonfun$buildReader$1.apply(AvroFileFormat.scala:215)
	at org.apache.spark.sql.avro.AvroFileFormat$$anonfun$buildReader$1.apply(AvroFileFormat.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)
	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2020-07-07 20:08:10,164 ERROR local variable 'data' referenced before assignment
Traceback (most recent call last):
  File "G:/Users/csaiamogh/PycharmProjects/RFM_Spark/src/main/python/main.py", line 108, in main
    write_obj.write_csv(df = data ,mode ='append',path = '/E:/linux/backup/Documents/og_tranx/customer_level_test/rfm_product_20191001-20191231/')
UnboundLocalError: local variable 'data' referenced before assignment
2020-07-07 20:08:10,165 ERROR local variable 'data_agg' referenced before assignment
Traceback (most recent call last):
  File "G:/Users/csaiamogh/PycharmProjects/RFM_Spark/src/main/python/main.py", line 112, in main
    write_obj.write_csv(df=data_agg, mode='append',
UnboundLocalError: local variable 'data_agg' referenced before assignment
2020-07-07 20:50:46,069 ERROR "java.net.URISyntaxException: Illegal character in scheme name at index 0: 'E:/linux/backup/Documents/og_tranx"
Traceback (most recent call last):
  File "C:\spark-2.4.6-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\utils.py", line 63, in deco
    return f(*a, **kw)
  File "C:\spark-2.4.6-bin-hadoop2.7\python\lib\py4j-0.10.7-src.zip\py4j\protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o27.load.
: java.lang.IllegalArgumentException: java.net.URISyntaxException: Illegal character in scheme name at index 0: 'E:/linux/backup/Documents/og_tranx
	at org.apache.hadoop.fs.Path.initialize(Path.java:205)
	at org.apache.hadoop.fs.Path.<init>(Path.java:171)
	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:546)
	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.URISyntaxException: Illegal character in scheme name at index 0: 'E:/linux/backup/Documents/og_tranx
	at java.net.URI$Parser.fail(URI.java:2847)
	at java.net.URI$Parser.checkChars(URI.java:3020)
	at java.net.URI$Parser.checkChar(URI.java:3030)
	at java.net.URI$Parser.parse(URI.java:3046)
	at java.net.URI.<init>(URI.java:746)
	at org.apache.hadoop.fs.Path.initialize(Path.java:202)
	... 24 more


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "G:/Users/csaiamogh/PycharmProjects/RFM_Spark/src/main/python/main.py", line 70, in main
    transactions_detail_csv = read_tables_obj.read_avro_file(file_path = file_path,file_format ='avro',spark=spark)
  File "src/main\read_write_modules\read.py", line 6, in read_avro_file
    df = spark.read.format(file_format).load(file_path)
  File "C:\spark-2.4.6-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\readwriter.py", line 166, in load
    return self._df(self._jreader.load(path))
  File "C:\spark-2.4.6-bin-hadoop2.7\python\lib\py4j-0.10.7-src.zip\py4j\java_gateway.py", line 1257, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "C:\spark-2.4.6-bin-hadoop2.7\python\lib\pyspark.zip\pyspark\sql\utils.py", line 79, in deco
    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)
pyspark.sql.utils.IllegalArgumentException: "java.net.URISyntaxException: Illegal character in scheme name at index 0: 'E:/linux/backup/Documents/og_tranx"
2020-07-07 20:50:46,108 ERROR local variable 'data' referenced before assignment
Traceback (most recent call last):
  File "G:/Users/csaiamogh/PycharmProjects/RFM_Spark/src/main/python/main.py", line 108, in main
    write_obj.write_csv(df = data ,mode ='append',path = '/E:/linux/backup/Documents/og_tranx/customer_level_test/rfm_product_20191001-20191231/')
UnboundLocalError: local variable 'data' referenced before assignment
2020-07-07 20:50:46,109 ERROR local variable 'data_agg' referenced before assignment
Traceback (most recent call last):
  File "G:/Users/csaiamogh/PycharmProjects/RFM_Spark/src/main/python/main.py", line 112, in main
    write_obj.write_csv(df=data_agg, mode='append',
UnboundLocalError: local variable 'data_agg' referenced before assignment
